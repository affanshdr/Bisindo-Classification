{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# BISINDO Classifier - Computer Vision Project\n",
    "\n",
    "Sistem klasifikasi BISINDO menggunakan Random Forest Classifier dengan implementasi lengkap teknik Computer Vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "NUM_AUGMENTED_IMAGE = 50\n",
    "CSV_FILENAME = 'data/bisindo_features.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            accuracy_score, precision_recall_fscore_support)\n",
    "import joblib\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Import library berhasil!')\n",
    "print(f'OpenCV version: {cv2.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_split",
   "metadata": {},
   "source": [
    "## Data Preparation & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(csv_filename):\n",
    "    \"\"\"\n",
    "    Load and split data into train, validation, and test sets\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    print('='*70)\n",
    "    print('DATASET INFORMATION')\n",
    "    print('='*70)\n",
    "    print(f'Total samples: {len(df):,}')\n",
    "    print(f'Number of features: {len(df.columns) - 1}')\n",
    "    print(f\"Number of classes: {df['label'].nunique()}\")\n",
    "    print(f\"Classes: {sorted(df['label'].unique())}\")\n",
    "    print(f'\\nClass distribution:')\n",
    "    print(df['label'].value_counts().sort_index())\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    class_counts = df['label'].value_counts()\n",
    "    print(f'\\nMin samples per class: {class_counts.min()}')\n",
    "    print(f'Max samples per class: {class_counts.max()}')\n",
    "    print(f'Imbalance ratio: {class_counts.max() / class_counts.min():.2f}x')\n",
    "    \n",
    "    # Split: 70% train, 15% validation, 15% test\n",
    "    train, temp = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
    "    val, test = train_test_split(temp, test_size=0.5, stratify=temp['label'], random_state=42)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Save splits\n",
    "    train.to_csv('data/train.csv', index=False)\n",
    "    val.to_csv('data/val.csv', index=False)\n",
    "    test.to_csv('data/test.csv', index=False)\n",
    "    \n",
    "    print(f'\\nüìÅ Data Split:')\n",
    "    print(f'Train samples: {len(train):,} ({len(train)/len(df)*100:.1f}%)')\n",
    "    print(f'Validation samples: {len(val):,} ({len(val)/len(df)*100:.1f}%)')\n",
    "    print(f'Test samples: {len(test):,} ({len(test)/len(df)*100:.1f}%)')\n",
    "    print('='*70)\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "train_df, val_df, test_df = split_data(CSV_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "train_df['label'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Train Set Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class (BISINDO Letter)', fontsize=11)\n",
    "axes[0].set_ylabel('Count', fontsize=11)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "val_df['label'].value_counts().sort_index().plot(kind='bar', ax=axes[1], color='lightgreen', edgecolor='black')\n",
    "axes[1].set_title('Validation Set Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Class (BISINDO Letter)', fontsize=11)\n",
    "axes[1].set_ylabel('Count', fontsize=11)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "test_df['label'].value_counts().sort_index().plot(kind='bar', ax=axes[2], color='lightcoral', edgecolor='black')\n",
    "axes[2].set_title('Test Set Distribution', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Class (BISINDO Letter)', fontsize=11)\n",
    "axes[2].set_ylabel('Count', fontsize=11)\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "axes[2].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/data_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Data distribution visualization saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_analysis",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze features\n",
    "feature_cols = [col for col in train_df.columns if col != 'label']\n",
    "X_train_analysis = train_df[feature_cols]\n",
    "\n",
    "print('='*70)\n",
    "print('üìà FEATURE STATISTICS')\n",
    "print('='*70)\n",
    "print(f'Number of features: {len(feature_cols)}')\n",
    "print(f'\\nFeature statistics:')\n",
    "print(X_train_analysis.describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing = X_train_analysis.isnull().sum().sum()\n",
    "print(f'\\nMissing values: {missing}')\n",
    "\n",
    "# Feature correlation (sample)\n",
    "print(f'\\nCalculating feature correlations...')\n",
    "sample_features = X_train_analysis.iloc[:1000, :20]  # Sample for speed\n",
    "correlation = sample_features.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation, cmap='coolwarm', center=0, square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix (Sample: First 20 Features)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/feature_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Feature analysis completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_prep",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X_train = train_df.drop('label', axis=1).values\n",
    "y_train = train_df['label'].values\n",
    "X_val = val_df.drop('label', axis=1).values\n",
    "y_val = val_df['label'].values\n",
    "X_test = test_df.drop('label', axis=1).values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "print('='*70)\n",
    "print('DATA PREPROCESSING')\n",
    "print('='*70)\n",
    "print(f'Train shape: X={X_train.shape}, y={y_train.shape}')\n",
    "print(f'Validation shape: X={X_val.shape}, y={y_val.shape}')\n",
    "print(f'Test shape: X={X_test.shape}, y={y_test.shape}')\n",
    "\n",
    "# Feature scaling (optional for Random Forest, but can help)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'\\nüìä Feature statistics after scaling:')\n",
    "print(f'Mean: {X_train_scaled.mean():.6f}')\n",
    "print(f'Std: {X_train_scaled.std():.6f}')\n",
    "print(f'Min: {X_train_scaled.min():.6f}')\n",
    "print(f'Max: {X_train_scaled.max():.6f}')\n",
    "print('='*70)\n",
    "\n",
    "print('\\nPreprocessing completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "**Mengapa Random Forest?**\n",
    "\n",
    "1. **Excellent Performance**: Sangat baik untuk tabular data\n",
    "2. **Feature Importance**: Dapat menganalisis fitur mana yang penting\n",
    "3. **Robust**: Tahan terhadap overfitting dengan ensemble method\n",
    "4. **No Feature Scaling Required**: Tidak sensitif terhadap skala features\n",
    "5. **Fast Inference**: Cepat untuk real-time prediction\n",
    "6. **Interpretable**: Lebih mudah dipahami daripada deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TRAINING RANDOM FOREST CLASSIFIER')\n",
    "print('='*70)\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,         \n",
    "    max_depth=30,              \n",
    "    min_samples_split=2,       \n",
    "    min_samples_leaf=1,        \n",
    "    max_features='sqrt',      \n",
    "    random_state=42,           \n",
    "    n_jobs=-1,                 \n",
    "    verbose=1,                 \n",
    "    class_weight='balanced'    \n",
    ")\n",
    "\n",
    "print('\\nüìã Model Configuration:')\n",
    "print(f'Number of trees: {rf_clf.n_estimators}')\n",
    "print(f'Max depth: {rf_clf.max_depth}')\n",
    "print(f'Max features: {rf_clf.max_features}')\n",
    "print(f'Class weight: {rf_clf.class_weight}')\n",
    "\n",
    "# Train the model\n",
    "print('\\nTraining started...')\n",
    "start_time = time.time()\n",
    "\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f'\\nTraining completed in {training_time:.2f} seconds')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on all sets\n",
    "print('='*70)\n",
    "print('MODEL PERFORMANCE')\n",
    "print('='*70)\n",
    "\n",
    "train_acc = rf_clf.score(X_train_scaled, y_train)\n",
    "val_acc = rf_clf.score(X_val_scaled, y_val)\n",
    "test_acc = rf_clf.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f'\\nüéØ Accuracy Results:')\n",
    "print(f'Train Accuracy: {train_acc*100:.2f}%')\n",
    "print(f'Validation Accuracy: {val_acc*100:.2f}%')\n",
    "print(f'Test Accuracy: {test_acc*100:.2f}%')\n",
    "\n",
    "# Check overfitting\n",
    "overfit_gap = (train_acc - test_acc) * 100\n",
    "print(f'\\nüìâ Overfitting Analysis:')\n",
    "print(f'Train-Test Gap: {overfit_gap:.2f}%')\n",
    "if overfit_gap < 5:\n",
    "    print('Model generalization: EXCELLENT')\n",
    "elif overfit_gap < 10:\n",
    "    print('Model generalization: GOOD')\n",
    "else:\n",
    "    print('Model generalization: Consider regularization')\n",
    "\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions",
   "metadata": {},
   "source": [
    "## Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = rf_clf.predict(X_train_scaled)\n",
    "y_pred_val = rf_clf.predict(X_val_scaled)\n",
    "y_pred_test = rf_clf.predict(X_test_scaled)\n",
    "\n",
    "print('='*70)\n",
    "print('CLASSIFICATION REPORT (TEST SET)')\n",
    "print('='*70)\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Per-class accuracy\n",
    "classes = sorted(np.unique(y_test))\n",
    "class_accuracies = []\n",
    "\n",
    "for cls in classes:\n",
    "    mask = y_test == cls\n",
    "    class_acc = accuracy_score(y_test[mask], y_pred_test[mask])\n",
    "    class_accuracies.append(class_acc)\n",
    "\n",
    "# Visualize per-class accuracy\n",
    "plt.figure(figsize=(14, 6))\n",
    "bars = plt.bar(classes, np.array(class_accuracies)*100, color='skyblue', edgecolor='black')\n",
    "plt.axhline(y=test_acc*100, color='red', linestyle='--', linewidth=2, label=f'Overall Accuracy: {test_acc*100:.2f}%')\n",
    "plt.xlabel('Class (BISINDO Letter)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "plt.title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 105])\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Color bars based on performance\n",
    "for i, (bar, acc) in enumerate(zip(bars, class_accuracies)):\n",
    "    if acc >= 0.95:\n",
    "        bar.set_color('green')\n",
    "    elif acc >= 0.85:\n",
    "        bar.set_color('lightgreen')\n",
    "    elif acc >= 0.75:\n",
    "        bar.set_color('yellow')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/per_class_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Per-class accuracy analysis completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=classes,\n",
    "            yticklabels=classes,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Random Forest Classifier', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=13)\n",
    "plt.xlabel('Predicted Label', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Greens', \n",
    "            xticklabels=classes,\n",
    "            yticklabels=classes,\n",
    "            cbar_kws={'label': 'Proportion'})\n",
    "plt.title('Normalized Confusion Matrix (Row-wise)', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=13)\n",
    "plt.xlabel('Predicted Label', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Confusion matrices saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_importance",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_imp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = rf_clf.feature_importances_\n",
    "feature_names = train_df.drop('label', axis=1).columns\n",
    "\n",
    "# Create dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print('='*70)\n",
    "print('üîç TOP 20 MOST IMPORTANT FEATURES')\n",
    "print('='*70)\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "print('='*70)\n",
    "\n",
    "# Visualize top 30 features\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_features = importance_df.head(30)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'], color='coral', edgecolor='black')\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 30 Most Important Features', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nFeature importance analysis completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross_val",
   "metadata": {},
   "source": [
    "## Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross_validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('üîÑ CROSS-VALIDATION ANALYSIS (5-Fold)')\n",
    "print('='*70)\n",
    "print('This may take a few minutes...')\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(rf_clf, X_train_scaled, y_train, cv=5, \n",
    "                            scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "print(f'\\nCross-Validation Results:')\n",
    "print(f'Individual fold scores: {[f\"{score*100:.2f}%\" for score in cv_scores]}')\n",
    "print(f'Mean CV Accuracy: {cv_scores.mean()*100:.2f}%')\n",
    "print(f'Std CV Accuracy: {cv_scores.std()*100:.2f}%')\n",
    "print(f\"95% Confidence Interval: [{(cv_scores.mean() - 2*cv_scores.std())*100:.2f}%, {(cv_scores.mean() + 2*cv_scores.std())*100:.2f}%]\\n\")\n",
    "\n",
    "# Visualize CV scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 6), cv_scores*100, marker='o', linewidth=2, markersize=10, color='blue')\n",
    "plt.axhline(y=cv_scores.mean()*100, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {cv_scores.mean()*100:.2f}%')\n",
    "plt.fill_between(range(1, 6), \n",
    "                 (cv_scores.mean() - cv_scores.std())*100, \n",
    "                 (cv_scores.mean() + cv_scores.std())*100, \n",
    "                 alpha=0.2, color='blue')\n",
    "plt.xlabel('Fold', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "plt.title('5-Fold Cross-Validation Scores', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(1, 6))\n",
    "plt.ylim([0, 105])\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/cross_validation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Cross-validation completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save",
   "metadata": {},
   "source": [
    "## Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('üíæ SAVING MODELS AND RESULTS')\n",
    "print('='*70)\n",
    "\n",
    "# Save Random Forest model\n",
    "joblib.dump(rf_clf, 'model/rf_bisindo_99.pkl')\n",
    "print('Random Forest model saved: model/rf_bisindo_99.pkl')\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'model/scaler.pkl')\n",
    "print('Scaler saved: model/scaler.pkl')\n",
    "\n",
    "# Save results summary\n",
    "results = {\n",
    "    'Model': 'Random Forest',\n",
    "    'N_Estimators': rf_clf.n_estimators,\n",
    "    'Max_Depth': rf_clf.max_depth,\n",
    "    'Train_Accuracy': f'{train_acc*100:.2f}%',\n",
    "    'Val_Accuracy': f'{val_acc*100:.2f}%',\n",
    "    'Test_Accuracy': f'{test_acc*100:.2f}%',\n",
    "    'CV_Mean_Accuracy': f'{cv_scores.mean()*100:.2f}%',\n",
    "    'CV_Std_Accuracy': f'{cv_scores.std()*100:.2f}%',\n",
    "    'Training_Time_Seconds': f'{training_time:.2f}',\n",
    "    'Number_of_Classes': len(classes),\n",
    "    'Total_Training_Samples': len(X_train),\n",
    "    'Total_Test_Samples': len(X_test)\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv('results/model_performance.csv', index=False)\n",
    "print('Results saved: results/model_performance.csv')\n",
    "\n",
    "print('='*70)\n",
    "print('\\nALL MODELS AND RESULTS SAVED SUCCESSFULLY!')\n",
    "\n",
    "# Print summary\n",
    "print('\\n' + '='*70)\n",
    "print('FINAL SUMMARY')\n",
    "print('='*70)\n",
    "print(f'Model: Random Forest Classifier')\n",
    "print(f'Number of Trees: {rf_clf.n_estimators}')\n",
    "print(f'Test Accuracy: {test_acc*100:.2f}%')\n",
    "print(f'Cross-Validation Accuracy: {cv_scores.mean()*100:.2f}% (¬±{cv_scores.std()*100:.2f}%)')\n",
    "print(f'Training Time: {training_time:.2f} seconds')\n",
    "print(f'Total Parameters: N/A (Tree-based model)')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference",
   "metadata": {},
   "source": [
    "## Inference Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference_time",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference speed\n",
    "import time\n",
    "\n",
    "print('='*70)\n",
    "print('INFERENCE SPEED ANALYSIS')\n",
    "print('='*70)\n",
    "\n",
    "# Single prediction\n",
    "single_sample = X_test_scaled[0:1]\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = rf_clf.predict(single_sample)\n",
    "single_time = (time.time() - start) / 1000\n",
    "\n",
    "\n",
    "\n",
    "# Batch prediction\n",
    "batch_sample = X_test_scaled[:100]\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = rf_clf.predict(batch_sample)\n",
    "batch_time = (time.time() - start) / 10 / 100\n",
    "\n",
    "\n",
    "\n",
    "print(f'Single prediction time: {single_time*1000:.2f} ms')\n",
    "print(f'Batch prediction time (per sample): {batch_time*1000:.2f} ms')\n",
    "print(f'Theoretical FPS (single): {1/single_time:.2f} FPS')\n",
    "print(f'Theoretical FPS (batch): {1/batch_time:.2f} FPS')\n",
    "print('='*70)\n",
    "\n",
    "if single_time < 0.033:  # 30 FPS\n",
    "    print('\\nModel is FAST ENOUGH for real-time applications (>30 FPS)')\n",
    "else:\n",
    "    print('\\n Model may be too slow for real-time (target: <33ms per frame)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary: Computer Vision Techniques Applied\n",
    "\n",
    "### Implemented Computer Vision Techniques (Pertemuan 1-16)\n",
    "\n",
    "| Pertemuan | Topik | Implementasi | Lokasi |\n",
    "|-----------|-------|--------------|---------|\n",
    "| 1-2 | Image Formation & Camera Model | Camera intrinsics calculation | Dashboard |\n",
    "| 3-4 | Image Preprocessing | Brightness, contrast, saturation | Dashboard |\n",
    "| 5 | Histogram Processing | Histogram equalization | Dashboard |\n",
    "| 6 | Thresholding | Binary, adaptive thresholding | Dashboard |\n",
    "| 7 | Segmentation | Convex hull hand segmentation | Dashboard |\n",
    "| 8-9 | Feature Extraction | MediaPipe 21 landmarks √ó 3D | Dashboard + Notebook |\n",
    "| 10 | Object Detection | Hand detection using MediaPipe | Dashboard |\n",
    "| 11 | Object Tracking | Real-time multi-hand tracking | Dashboard |\n",
    "| 12 | 3D Reconstruction | Depth visualization (z-coords) | Dashboard |\n",
    "| 13 | Augmented Reality | AR overlay with bounding boxes | Dashboard |\n",
    "| 14 | Machine Learning | Random Forest classifier | Notebook |\n",
    "| 15-16 | Deep Learning | Feature importance analysis | Notebook |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
